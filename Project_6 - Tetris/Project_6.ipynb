{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This Python 3 environment comes with many helpful analytics libraries installed\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# For example, here's several helpful packages to load\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Input data files are available in the read-only \"../input/\" directory\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dirname, _, filenames \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m.walk(\u001b[33m\"\u001b[39m\u001b[33m./\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(os.path.join(dirname, filename))\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of this project is to make a self-playing Tetris game that is managed by a model that attempts to survive as long as possibele. Using six different tetris pieces, each with four different possible rientations, my objective is to clear out horizontal lines before letting the stack of blocks hit a certain height.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The purpose of this project is to make a self-playing Tetris game that is managed by a model that attempts to survive as long as possibele. Using six different tetris pieces, each with four different possible rientations, my objective is to clear out horizontal lines before letting the stack of blocks hit a certain height.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ultimately, this project differs greatly from others in many senses of the typicaly ML workflow. In thsi case, I am using reinforcement learning, so the data that I have is necessarily the times that I end up running a certain type of model. For now, the only data that I have access to are a few models that have been provided to me in the form of the already created Tetris game, making it so that I didn't have to code the whole framework myself for the GUI.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ultimately, this project differs greatly from others in many senses of the typicaly ML workflow. In thsi case, I am using reinforcement learning, so the data that I have is necessarily the times that I end up running a certain type of model. For now, the only data that I have access to are a few models that have been provided to me in the form of the already created Tetris game, making it so that I didn't have to code the whole framework myself for the GUI.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can\\'t necessarily explore the given \"dataset\" per se, but I can still try to play the game to see what works and doesn\\'t work best. By understanding this, I can see what types of features make a good model in comparison to a poorly performing one. So far, the repository we were given has 4 different models to test. A genetic algorithm which performs poorly, a random algorithm which also performs badly, a monte carlo tree search algorithm which doesn\\'t perform that great, but finally, a greedy algorithm which performs very well. After trying these different models on the game, and visualizing them in Pycharm, I came to the conclusion that the files of board.py managed the board, piece.py managed the falling piece, and game.py tied everything together.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"I can't necessarily explore the given \"dataset\" per se, but I can still try to play the game to see what works and doesn't work best. By understanding this, I can see what types of features make a good model in comparison to a poorly performing one. So far, the repository we were given has 4 different models to test. A genetic algorithm which performs poorly, a random algorithm which also performs badly, a monte carlo tree search algorithm which doesn't perform that great, but finally, a greedy algorithm which performs very well. After trying these different models on the game, and visualizing them in Pycharm, I came to the conclusion that the files of board.py managed the board, piece.py managed the falling piece, and game.py tied everything together.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There isn't necessarily any data to be preprocessed, because the outcomes of the games or the gameplay itself isn't stored in a database. Once I actually begin modeling, though, using RL, I will have some data that can be managed.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"There isn't necessarily any data to be preprocessed, because the outcomes of the games or the gameplay itself isn't stored in a database. Once I actually begin modeling, though, using RL, I will have some data that can be managed.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from genetic_helpers import (\n",
    "    get_peaks, get_holes, get_wells,\n",
    "    get_bumpiness, get_row_transition, get_col_transition, bool_to_np\n",
    ")\n",
    "from board import Board\n",
    "from piece import Piece, BODIES\n",
    "\n",
    "\n",
    "class HybridTetrisAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=9,\n",
    "        alpha=0.01,\n",
    "        gamma=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.05,\n",
    "        epsilon_decay=0.9997\n",
    "    ):\n",
    "        self.weights = np.random.uniform(-0.01, 0.01, num_features)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def extract_features(self, board_np):\n",
    "        peaks = get_peaks(board_np)\n",
    "        highest_peak = np.max(peaks)\n",
    "        holes = get_holes(peaks, board_np)\n",
    "        wells = get_wells(peaks)\n",
    "\n",
    "        return np.array([\n",
    "            np.sum(peaks),\n",
    "            np.sum(holes),\n",
    "            get_bumpiness(peaks),\n",
    "            np.count_nonzero(np.count_nonzero(board_np, axis=0) == 0),\n",
    "            np.max(wells),\n",
    "            np.count_nonzero(np.array(holes) > 0),\n",
    "            get_row_transition(board_np, highest_peak),\n",
    "            get_col_transition(board_np, peaks),\n",
    "            np.sum(np.count_nonzero(board_np, axis=1))\n",
    "        ], dtype=float)\n",
    "\n",
    "    def q_value(self, features):\n",
    "        return float(np.dot(self.weights, features))\n",
    "\n",
    "    def choose_action(self, board, piece):\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            rot_count = random.randint(0, 3)\n",
    "            temp_piece = piece\n",
    "            for _ in range(rot_count):\n",
    "                temp_piece = temp_piece.get_next_rotation()\n",
    "            max_x = board.width - len(temp_piece.skirt)\n",
    "            x = random.randint(0, max_x)\n",
    "            return x, temp_piece\n",
    "\n",
    "        best_value = -1e18\n",
    "        best_x = None\n",
    "        best_piece = None\n",
    "\n",
    "        rot_piece = piece\n",
    "        for _ in range(4):\n",
    "            rot_piece = rot_piece.get_next_rotation()\n",
    "            max_x = board.width - len(rot_piece.skirt)\n",
    "\n",
    "            for x in range(max_x + 1):\n",
    "                try:\n",
    "                    y = board.drop_height(rot_piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # simulate placement\n",
    "                grid = deepcopy(board.board)\n",
    "                for pos in rot_piece.body:\n",
    "                    grid[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                grid_np = bool_to_np(grid)\n",
    "                f = self.extract_features(grid_np)\n",
    "                v = self.q_value(f)\n",
    "\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    best_x = x\n",
    "                    best_piece = rot_piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def update(self, prev_features, reward, next_features):\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        prev_q = self.q_value(prev_features)\n",
    "        next_q = self.q_value(next_features)\n",
    "        if np.isnan(prev_q): prev_q = 0\n",
    "        if np.isnan(next_q): next_q = 0\n",
    "\n",
    "        td_target = reward + self.gamma * next_q\n",
    "        td_error = td_target - prev_q\n",
    "        td_error = np.clip(td_error, -5, 5)\n",
    "\n",
    "        self.weights += self.alpha * td_error * prev_features\n",
    "        self.weights = np.nan_to_num(self.weights)\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "def train_hybrid_agent(episodes=5000):\n",
    "    agent = HybridTetrisAgent()\n",
    "    print(\"Starting Hybrid Tetris training...\")\n",
    "\n",
    "    best_moves = 0\n",
    "    last50_moves = []\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        board = Board()\n",
    "        total_reward = 0\n",
    "        moves = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            piece = Piece(body=random.choice(BODIES)[0])\n",
    "            prev_board_np = bool_to_np(board.board)\n",
    "            prev_features = agent.extract_features(prev_board_np)\n",
    "\n",
    "            x, piece_used = agent.choose_action(board, piece)\n",
    "            if x is None or piece_used is None:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            # Place piece\n",
    "            try:\n",
    "                y = board.drop_height(piece_used, x)\n",
    "                board.place(x, y, piece_used)\n",
    "            except:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            cleared = board.clear_rows()\n",
    "            next_board_np = bool_to_np(board.board)\n",
    "            next_features = agent.extract_features(next_board_np)\n",
    "\n",
    "            holes_next = np.sum(get_holes(get_peaks(next_board_np), next_board_np))\n",
    "            if cleared > 0:\n",
    "                reward = 1\n",
    "            elif holes_next > 0:\n",
    "                reward = -0.05\n",
    "            else:\n",
    "                reward = -0.01\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.update(prev_features, reward, next_features)\n",
    "            moves += 1\n",
    "\n",
    "            if np.any(board.heights >= board.height):\n",
    "                done = True\n",
    "                \n",
    "        last50_moves.append(moves)\n",
    "        if len(last50_moves) > 50:\n",
    "            last50_moves.pop(0)\n",
    "\n",
    "        avg_last50 = np.mean(last50_moves)\n",
    "        best_moves = max(best_moves, moves)\n",
    "\n",
    "        print(\n",
    "            f\"Gen {episode:4d}/{episodes} | \"\n",
    "            f\"Moves: {moves:4d} | \"\n",
    "            f\"Best: {best_moves:4d} | \"\n",
    "            f\"Avg50: {avg_last50:6.2f} | \"\n",
    "            f\"Reward: {total_reward:7.3f} | \"\n",
    "            f\"Eps: {agent.epsilon:.3f}\"\n",
    "        )\n",
    "\n",
    "    np.save(\"trained_weights.npy\", agent.weights)\n",
    "    print(\"Training complete! Saved weights to trained_weights.npy\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_hybrid_agent(episodes=1000)\n",
    "\n",
    "\n",
    "\"\"\"Here is just the custom_model.py that I ended up using\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
