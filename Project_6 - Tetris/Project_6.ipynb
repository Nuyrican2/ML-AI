{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path where custom_model.py is located\n",
    "sys.path.insert(0, '/home/jupyter-1000790/ML-AI/Project_6 - Tetris/tetris_a/src/custom_model.py')\n",
    "\n",
    "# Or if it's in a subdirectory like tetris_a/src:\n",
    "notebook_dir = os.getcwd()\n",
    "tetris_src_path = os.path.join(notebook_dir, \"tetris_a\", \"src\")\n",
    "sys.path.insert(0, tetris_src_path)\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of this project is to make a self-playing Tetris game that is managed by a model that attempts to survive as long as possibele. Using six different tetris pieces, each with four different possible rientations, my objective is to clear out horizontal lines before letting the stack of blocks hit a certain height.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The purpose of this project is to make a self-playing Tetris game that is managed by a model that attempts to survive as long as possibele. Using six different tetris pieces, each with four different possible rientations, my objective is to clear out horizontal lines before letting the stack of blocks hit a certain height.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ultimately, this project differs greatly from others in many senses of the typicaly ML workflow. In thsi case, I am using reinforcement learning, so the data that I have is necessarily the times that I end up running a certain type of model. For now, the only data that I have access to are a few models that have been provided to me in the form of the already created Tetris game, making it so that I didn't have to code the whole framework myself for the GUI.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ultimately, this project differs greatly from others in many senses of the typicaly ML workflow. In thsi case, I am using reinforcement learning, so the data that I have is necessarily the times that I end up running a certain type of model. For now, the only data that I have access to are a few models that have been provided to me in the form of the already created Tetris game, making it so that I didn't have to code the whole framework myself for the GUI.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can\\'t necessarily explore the given \"dataset\" per se, but I can still try to play the game to see what works and doesn\\'t work best. By understanding this, I can see what types of features make a good model in comparison to a poorly performing one. So far, the repository we were given has 4 different models to test. A genetic algorithm which performs poorly, a random algorithm which also performs badly, a monte carlo tree search algorithm which doesn\\'t perform that great, but finally, a greedy algorithm which performs very well. After trying these different models on the game, and visualizing them in Pycharm, I came to the conclusion that the files of board.py managed the board, piece.py managed the falling piece, and game.py tied everything together.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"I can't necessarily explore the given \"dataset\" per se, but I can still try to play the game to see what works and doesn't work best. By understanding this, I can see what types of features make a good model in comparison to a poorly performing one. So far, the repository we were given has 4 different models to test. A genetic algorithm which performs poorly, a random algorithm which also performs badly, a monte carlo tree search algorithm which doesn't perform that great, but finally, a greedy algorithm which performs very well. After trying these different models on the game, and visualizing them in Pycharm, I came to the conclusion that the files of board.py managed the board, piece.py managed the falling piece, and game.py tied everything together.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There isn't necessarily any data to be preprocessed, because the outcomes of the games or the gameplay itself isn't stored in a database. Once I actually begin modeling, though, using RL, I will have some data that can be managed.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"There isn't necessarily any data to be preprocessed, because the outcomes of the games or the gameplay itself isn't stored in a database. Once I actually begin modeling, though, using RL, I will have some data that can be managed.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I first thought that I should try a RL model, but at first the way that I constructed it, was that my rewards were based on different parameters. I then drew inspiration from both the greedy and genetic models, and figured out the specific criteria like bumpiness and holes which I should apply rewards to. After doing so, the model improved from not learning at all, to being able to advance to around 1000 lines cleared on average after training for a few hours. Ultimately, I decided to continue with this model, and saved it to a pickle file, since the way that I conducted my training, is that it saved the best model through each generation to a pickle file. This final model embodied a specific type of model which is an evolutionary heuristic model based on different weights, which adapt to model efficiency generation by generation.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"I first thought that I should try a RL model, but at first the way that I constructed it, was that my rewards were based on different parameters. I then drew inspiration from both the greedy and genetic models, and figured out the specific criteria like bumpiness and holes which I should apply rewards to. After doing so, the model improved from not learning at all, to being able to advance to around 1000 lines cleared on average after training for a few hours. Ultimately, I decided to continue with this model, and saved it to a pickle file, since the way that I conducted my training, is that it saved the best model through each generation to a pickle file. This final model embodied a specific type of model which is an evolutionary heuristic model based on different weights, which adapt to model efficiency generation by generation.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 204) (4012437259.py, line 204)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 204)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from genetic_helpers import (\n",
    "    get_peaks, get_holes, get_wells,\n",
    "    get_bumpiness, get_row_transition, get_col_transition, bool_to_np\n",
    ")\n",
    "from board import Board\n",
    "from piece import Piece, BODIES\n",
    "\n",
    "\n",
    "class HybridTetrisAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=9,\n",
    "        alpha=0.02,  # slightly higher learning rate\n",
    "        gamma=0.97,  # slightly higher discount\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.05,\n",
    "        epsilon_decay=0.9995  # slower decay for more exploration early\n",
    "    ):\n",
    "        self.weights = np.random.uniform(-0.01, 0.01, num_features)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def extract_features(self, board_np):\n",
    "        peaks = get_peaks(board_np)\n",
    "        holes = get_holes(peaks, board_np)\n",
    "        wells = get_wells(peaks)\n",
    "\n",
    "        # normalize features to roughly similar scale\n",
    "        peaks_sum = np.sum(peaks) / 20\n",
    "        holes_sum = np.sum(holes) / 10\n",
    "        bumpiness = get_bumpiness(peaks) / 10\n",
    "        empty_cols = np.count_nonzero(np.count_nonzero(board_np, axis=0) == 0) / board_np.shape[1]\n",
    "        max_wells = np.max(wells) / 10\n",
    "        cols_with_holes = np.count_nonzero(np.array(holes) > 0) / board_np.shape[1]\n",
    "        row_trans = get_row_transition(board_np, np.max(peaks)) / board_np.shape[1]\n",
    "        col_trans = get_col_transition(board_np, peaks) / board_np.shape[1]\n",
    "        rows_filled = np.sum(np.count_nonzero(board_np, axis=1)) / board_np.shape[0]\n",
    "\n",
    "        return np.array([\n",
    "            peaks_sum,\n",
    "            holes_sum,\n",
    "            bumpiness,\n",
    "            empty_cols,\n",
    "            max_wells,\n",
    "            cols_with_holes,\n",
    "            row_trans,\n",
    "            col_trans,\n",
    "            rows_filled\n",
    "        ], dtype=float)\n",
    "\n",
    "    def q_value(self, features):\n",
    "        return float(np.dot(self.weights, features))\n",
    "\n",
    "    def choose_action(self, board, piece, lookahead=2):\n",
    "        # epsilon-greedy\n",
    "        if random.random() < self.epsilon:\n",
    "            rot_count = random.randint(0, 3)\n",
    "            temp_piece = piece\n",
    "            for _ in range(rot_count):\n",
    "                temp_piece = temp_piece.get_next_rotation()\n",
    "            max_x = board.width - len(temp_piece.skirt)\n",
    "            x = random.randint(0, max_x)\n",
    "            return x, temp_piece\n",
    "\n",
    "        best_value = -1e18\n",
    "        best_x = None\n",
    "        best_piece = None\n",
    "\n",
    "        rot_piece = piece\n",
    "        for _ in range(4):\n",
    "            rot_piece = rot_piece.get_next_rotation()\n",
    "            max_x = board.width - len(rot_piece.skirt)\n",
    "            for x in range(max_x + 1):\n",
    "                try:\n",
    "                    y = board.drop_height(rot_piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # simulate placement\n",
    "                grid = deepcopy(board.board)\n",
    "                for pos in rot_piece.body:\n",
    "                    grid[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                # lookahead: average over a few random next pieces\n",
    "                lookahead_value = 0\n",
    "                for _ in range(lookahead):\n",
    "                    next_piece = Piece(body=random.choice(BODIES)[0])\n",
    "                    next_rot = next_piece.get_next_rotation()\n",
    "                    max_x2 = board.width - len(next_rot.skirt)\n",
    "                    x2 = random.randint(0, max_x2)\n",
    "                    y2 = 0\n",
    "                    try:\n",
    "                        y2 = board.drop_height(next_rot, x2)\n",
    "                    except:\n",
    "                        continue\n",
    "                    grid2 = deepcopy(grid)\n",
    "                    for pos in next_rot.body:\n",
    "                        grid2[y2 + pos[1]][x2 + pos[0]] = True\n",
    "                    f2 = self.extract_features(bool_to_np(grid2))\n",
    "                    lookahead_value += self.q_value(f2)\n",
    "                lookahead_value /= lookahead\n",
    "\n",
    "                f = self.extract_features(bool_to_np(grid))\n",
    "                v = self.q_value(f) + 0.5 * lookahead_value  # weight lookahead\n",
    "\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    best_x = x\n",
    "                    best_piece = rot_piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "    def update(self, prev_features, reward, next_features):\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        prev_q = self.q_value(prev_features)\n",
    "        next_q = self.q_value(next_features)\n",
    "        td_target = reward + self.gamma * next_q\n",
    "        td_error = td_target - prev_q\n",
    "        td_error = np.clip(td_error, -10, 10)  # allow slightly bigger updates\n",
    "        self.weights += self.alpha * td_error * prev_features\n",
    "        self.weights = np.nan_to_num(self.weights)\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "def train_hybrid_agent(episodes=1000):\n",
    "    agent = HybridTetrisAgent()\n",
    "    print(\"Starting Hybrid Tetris training...\")\n",
    "\n",
    "    best_moves = 0\n",
    "    last50_moves = []\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        board = Board()\n",
    "        total_reward = 0\n",
    "        moves = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            piece = Piece(body=random.choice(BODIES)[0])\n",
    "            prev_board_np = bool_to_np(board.board)\n",
    "            prev_features = agent.extract_features(prev_board_np)\n",
    "\n",
    "            x, piece_used = agent.choose_action(board, piece)\n",
    "            if x is None or piece_used is None:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                y = board.drop_height(piece_used, x)\n",
    "                board.place(x, y, piece_used)\n",
    "            except:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            cleared = board.clear_rows()\n",
    "            next_board_np = bool_to_np(board.board)\n",
    "            next_features = agent.extract_features(next_board_np)\n",
    "\n",
    "            holes_next = np.sum(get_holes(get_peaks(next_board_np), next_board_np))\n",
    "            height_penalty = np.max(get_peaks(next_board_np)) / 20\n",
    "\n",
    "            reward = cleared * 1.0 - holes_next * 0.05 - height_penalty * 0.02\n",
    "\n",
    "            total_reward += reward\n",
    "            agent.update(prev_features, reward, next_features)\n",
    "            moves += 1\n",
    "\n",
    "            if np.any(board.heights >= board.height):\n",
    "                done = True\n",
    "\n",
    "        last50_moves.append(moves)\n",
    "        if len(last50_moves) > 50:\n",
    "            last50_moves.pop(0)\n",
    "        avg_last50 = np.mean(last50_moves)\n",
    "        best_moves = max(best_moves, moves)\n",
    "\n",
    "        print(\n",
    "            f\"Gen {episode:4d}/{episodes} | \"\n",
    "            f\"Moves: {moves:4d} | \"\n",
    "            f\"Best: {best_moves:4d} | \"\n",
    "            f\"Avg50: {avg_last50:6.2f} | \"\n",
    "            f\"Reward: {total_reward:7.3f} | \"\n",
    "            f\"Eps: {agent.epsilon:.3f}\"\n",
    "        )\n",
    "\n",
    "    np.save(\"trained_weights.npy\", agent.weights)\n",
    "    print(\"Training complete! Saved weights to trained_weights.npy\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_hybrid_agent(episodes=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'\n",
    "\"\"\"I improved the model by slightly increasing alpha for faster learning and raising gamma to value long-term rewards. Epsilon decays slower to encourage early exploration, while feature normalization stabilizes updates. A lookahead mechanism evaluates candidate placements with a few upcoming pieces for smarter decisions, and reward shaping—based on cleared lines, holes, and max height—guides the agent toward more effective play.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ultimately, this project had a specific approach which revolved around one main idea. Essentially, through the use of reinforcement learning, I \\nplanned to make my model learn as I trained it. So far, it has continued to be trained, and its accuracy in playing the tetris game continues to improve. \\nSpecificaly, though, I used some of the greedy and genetic models which had certain benefits, mixed them together, and created a product which was\\nbetter than before. The genetic model made it so that the model could keep getting better, whlie greedy was already good from the get go. I then fine\\ntuned this model, so that the epsilon value would decraese slower, and the testing would be conducted in a better way. I concluded that the evolutionary \\nmodel with heuristic weights actually worked the best, because in comparison to the first RL model I tried, the model actually learned from its mistakes,\\nand the rewards that were given improved its efficiency.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ultimately, this project had a specific approach which revolved around one main idea. Essentially, through the use of reinforcement learning, I \n",
    "planned to make my model learn as I trained it. So far, it has continued to be trained, and its accuracy in playing the tetris game continues to improve. \n",
    "Specificaly, though, I used some of the greedy and genetic models which had certain benefits, mixed them together, and created a product which was\n",
    "better than before. The genetic model made it so that the model could keep getting better, whlie greedy was already good from the get go. I then fine\n",
    "tuned this model, so that the epsilon value would decraese slower, and the testing would be conducted in a better way. I concluded that the evolutionary \n",
    "model with heuristic weights actually worked the best, because in comparison to the first RL model I tried, the model actually learned from its mistakes,\n",
    "and the rewards that were given improved its efficiency.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best_agent_fixed.pkl\n",
      "Trained weights: [ 0.81871496  1.58361802  0.38366396 -0.68594702]\n",
      "\n",
      "============================================================\n",
      "Testing trained agent...\n",
      "============================================================\n",
      "Progress: 100 pieces, 42 lines cleared\n",
      "Progress: 200 pieces, 83 lines cleared\n",
      "Progress: 300 pieces, 125 lines cleared\n",
      "Progress: 400 pieces, 166 lines cleared\n",
      "Progress: 500 pieces, 209 lines cleared\n",
      "Progress: 600 pieces, 245 lines cleared\n",
      "Progress: 700 pieces, 295 lines cleared\n",
      "Progress: 800 pieces, 338 lines cleared\n",
      "Progress: 900 pieces, 379 lines cleared\n",
      "Progress: 1000 pieces, 419 lines cleared\n",
      "Progress: 1100 pieces, 466 lines cleared\n",
      "Progress: 1200 pieces, 505 lines cleared\n",
      "Progress: 1300 pieces, 547 lines cleared\n",
      "Progress: 1400 pieces, 589 lines cleared\n",
      "Progress: 1500 pieces, 629 lines cleared\n",
      "Progress: 1600 pieces, 670 lines cleared\n",
      "Progress: 1700 pieces, 717 lines cleared\n",
      "Progress: 1800 pieces, 758 lines cleared\n",
      "Progress: 1900 pieces, 805 lines cleared\n",
      "Progress: 2000 pieces, 846 lines cleared\n",
      "Progress: 2100 pieces, 885 lines cleared\n",
      "Progress: 2200 pieces, 924 lines cleared\n",
      "Progress: 2300 pieces, 972 lines cleared\n",
      "Progress: 2400 pieces, 1007 lines cleared\n",
      "Progress: 2500 pieces, 1058 lines cleared\n",
      "Progress: 2600 pieces, 1097 lines cleared\n",
      "Progress: 2700 pieces, 1144 lines cleared\n",
      "Progress: 2800 pieces, 1185 lines cleared\n",
      "Progress: 2900 pieces, 1225 lines cleared\n",
      "Progress: 3000 pieces, 1267 lines cleared\n",
      "Progress: 3100 pieces, 1310 lines cleared\n",
      "Progress: 3200 pieces, 1353 lines cleared\n",
      "Progress: 3300 pieces, 1395 lines cleared\n",
      "Progress: 3400 pieces, 1440 lines cleared\n",
      "Progress: 3500 pieces, 1482 lines cleared\n",
      "Progress: 3600 pieces, 1526 lines cleared\n",
      "Progress: 3700 pieces, 1569 lines cleared\n",
      "Progress: 3800 pieces, 1611 lines cleared\n",
      "Progress: 3900 pieces, 1652 lines cleared\n",
      "Progress: 4000 pieces, 1694 lines cleared\n",
      "Progress: 4100 pieces, 1735 lines cleared\n",
      "Progress: 4200 pieces, 1780 lines cleared\n",
      "Progress: 4300 pieces, 1817 lines cleared\n"
     ]
    }
   ],
   "source": [
    "def inference(checkpoint_file=\"best_agent.pkl\", max_pieces=100000, print_progress=True):\n",
    "    \n",
    "    import pickle\n",
    "    from custom_model import EvolutionaryAgent\n",
    "    from board import Board\n",
    "    from piece import Piece\n",
    "   \n",
    "    try:\n",
    "        with open(checkpoint_file, \"rb\") as f:\n",
    "            trained_agent = pickle.load(f)\n",
    "        print(\"Loaded\", checkpoint_file)\n",
    "        print(f\"Trained weights: {trained_agent.weights}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No {checkpoint_file} found - make sure you ran training first!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing trained agent...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    board = Board()\n",
    "    pieces_placed = 0\n",
    "    lines_cleared = 0\n",
    "    \n",
    "    while pieces_placed < max_pieces:\n",
    "        piece = Piece()\n",
    "        x, piece = trained_agent.get_best_move(board, piece)\n",
    "        \n",
    "        if x is None:\n",
    "            break\n",
    "        \n",
    "        y = board.drop_height(piece, x)\n",
    "        \n",
    "        try:\n",
    "            board.place(x, y, piece)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        cleared = board.clear_rows()\n",
    "        lines_cleared += cleared\n",
    "        pieces_placed += 1\n",
    "        \n",
    "        if print_progress and pieces_placed % 100 == 0:\n",
    "            print(f\"Progress: {pieces_placed} pieces, {lines_cleared} lines cleared\")\n",
    "        \n",
    "        if board.top_filled():\n",
    "            break\n",
    "    \n",
    "    score = pieces_placed + (lines_cleared * 10)\n",
    "    \n",
    "    print(f\"\\nGame complete!\")\n",
    "    print(f\"Pieces placed: {pieces_placed}\")\n",
    "    print(f\"Lines cleared: {lines_cleared}\")\n",
    "    print(f\"Score: {score:.1f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return score, pieces_placed, lines_cleared\n",
    "\n",
    "score, pieces, lines = inference(\"best_agent_fixed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
